{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "#import umap  # depend de si python=3.9 ou pas\n",
    "import umap.umap_ as umap\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import ConvergenceError\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = \"../features_with_clinical_data_KNN.csv\"\n",
    "num_bootstraps = 1000\n",
    "NUMBER_OF_FEATURES = range(1, 11)\n",
    "penalizer = [1e-5, 1e-4, 1e-3, 1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_)\n",
    "df.drop([\"weight\"], inplace=True, axis=1)\n",
    "df = df.astype(np.float32)\n",
    "df.set_index(\"patient_id\", inplace=True)\n",
    "df.drop([\"PFS\"], inplace=True, axis=1)\n",
    "df.rename(columns={\"Follow-up Status\": \"FUS\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [\"C\" + str(i) for i in range(df.shape[1]-19)]\n",
    "first_cols = ['MYC IHC', 'BCL2 IHC', 'BCL6 IHC', 'CD10 IHC', 'MUM1 IHC',\n",
    "        'HANS', 'BCL6 FISH', 'MYC FISH', 'BCL2 FISH', 'Age', 'ECOG PS', 'LDH',\n",
    "        'EN', 'Stage', 'IPI Score', 'IPI Risk Group (4 Class)', 'RIPI Risk Group', 'OS', 'FUS']\n",
    "\n",
    "df.columns = first_cols + last_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarianceMethod(df, n_feats):\n",
    "    new_df = df[last_cols]\n",
    "    std = []\n",
    "    for col in new_df.columns:\n",
    "        std.append(df[col].std())\n",
    "    \n",
    "    std = np.array(std)\n",
    "    std_top_n = np.argsort(std)[-n_feats:]\n",
    "    std_top_n_df = new_df.iloc[:, std_top_n]\n",
    "    final_df = pd.concat([df[first_cols], std_top_n_df], axis=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAMethod(df, n_feats):\n",
    "    new_df = df[last_cols]\n",
    "    pca = PCA(n_components=n_feats)\n",
    "    pca.fit(new_df)\n",
    "    pca_transformed = pca.transform(new_df)\n",
    "    pca_df = pd.DataFrame(data=pca_transformed, columns=['PC{}'.format(i+1) for i in range(n_feats)])\n",
    "    final_df = np.hstack((df[first_cols], pca_df))\n",
    "    final_df= pd.DataFrame(final_df, columns=list(df[first_cols].columns) + list(pca_df.columns))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICAMethod(df, n_feats):\n",
    "    new_df = df[last_cols]\n",
    "    ica = FastICA(n_components=n_feats)\n",
    "    ica.fit(new_df)\n",
    "    ica_transformed = ica.transform(new_df)\n",
    "    ica_df = pd.DataFrame(data=ica_transformed, columns=['IC{}'.format(i+1) for i in range(n_feats)])\n",
    "    final_df = np.hstack((df[first_cols], ica_df))\n",
    "    final_df= pd.DataFrame(final_df, columns=list(df[first_cols].columns) + list(ica_df.columns))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UMAPMethod(df, n_feats):\n",
    "    new_df = df[last_cols]\n",
    "    reducer = umap.UMAP(n_components=n_feats)\n",
    "    reducer.fit(new_df)\n",
    "    umap_transformed = reducer.transform(new_df)\n",
    "    umap_df = pd.DataFrame(data=umap_transformed, columns=['UMAP{}'.format(i+1) for i in range(n_feats)])\n",
    "    final_df = np.hstack((df[first_cols], umap_df))\n",
    "    final_df = pd.DataFrame(final_df, columns=list(df[first_cols].columns) + list(umap_df.columns))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMapMethod(df, n_feats):\n",
    "    new_df = df[last_cols]\n",
    "    iso = Isomap(n_components=n_feats)\n",
    "    iso.fit(new_df)\n",
    "    iso_transformed = iso.transform(new_df)\n",
    "    iso_df = pd.DataFrame(data=iso_transformed, columns=['ISO{}'.format(i+1) for i in range(n_feats)])\n",
    "    final_df = np.hstack((df[first_cols], iso_df))\n",
    "    final_df= pd.DataFrame(final_df, columns=list(df[first_cols].columns) + list(iso_df.columns))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_c_index(final_df, p):\n",
    "    c_b_boot, c_b_orig = [], []\n",
    "    bootstrap_size = len(final_df)\n",
    "\n",
    "    for i in range(num_bootstraps):\n",
    "        choices = np.random.choice(np.arange(0, len(final_df)), size=bootstrap_size, replace=True)\n",
    "        new_df = final_df.iloc[choices]  #sample bootstrap replicate with replacement\n",
    "\n",
    "        cph = CoxPHFitter(penalizer=p)\n",
    "        cph.fit(new_df, duration_col='OS', event_col='FUS')  #fit on bootstrap\n",
    "\n",
    "        c = cph.score(new_df, scoring_method=\"concordance_index\")  #score on bootstrap\n",
    "        c_b_boot.append(c)\n",
    "\n",
    "        c = cph.score(final_df, scoring_method=\"concordance_index\")  #score on original\n",
    "        c_b_orig.append(c)\n",
    "\n",
    "    c_b_orig = np.array(c_b_orig)\n",
    "    c_b_boot = np.array(c_b_boot)\n",
    "    error = np.mean(c_b_boot - c_b_orig)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_feats in NUMBER_OF_FEATURES:\n",
    "    print('n features:', 2**n_feats)\n",
    "    for p in penalizer:\n",
    "        for method, name in tqdm.tqdm((\n",
    "            (VarianceMethod, 'variance'),\n",
    "            (PCAMethod, 'pca'),\n",
    "            (ICAMethod, 'ica'),\n",
    "            (UMAPMethod, 'umap'),\n",
    "            (IsoMapMethod, 'isomap'))):\n",
    "\n",
    "            try:\n",
    "                new_df = method(df, 2**n_feats)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                results.append(dict(method=name, n_feats=n_feats, cindex=None, cindex_opt=None, error=None, hi=None, lo=None, ebm=None))\n",
    "                continue\n",
    "            \n",
    "            bootstrap_size = len(new_df)\n",
    "\n",
    "            qt = QuantileTransformer(n_quantiles=10, random_state=42)\n",
    "            qt.fit(new_df)\n",
    "            new_df = pd.DataFrame.from_records(qt.transform(new_df), columns=new_df.columns)\n",
    "\n",
    "            cph = CoxPHFitter(penalizer=p)\n",
    "            try:\n",
    "                cph.fit(new_df, duration_col='OS', event_col='FUS')\n",
    "            except ConvergenceError:\n",
    "                results.append(dict(method=name, n_feats=n_feats, cindex=None, cindex_opt=None, error=None, hi=None, lo=None, ebm=None))\n",
    "                continue\n",
    "            \n",
    "            c_main = cph.score(new_df, scoring_method=\"concordance_index\")\n",
    "\n",
    "            try:\n",
    "                error = corrected_c_index(new_df, p)\n",
    "            except ValueError:\n",
    "                results.append(dict(method=name, n_feats=n_feats, cindex=c_main, cindex_opt=None, error=None, hi=None, lo=None, ebm=None))\n",
    "                continue\n",
    "            \n",
    "            c_corrected = c_main - error\n",
    "\n",
    "            c_indices = []\n",
    "            for i in range(num_bootstraps):\n",
    "                choices = np.random.choice(np.arange(0, len(new_df)), size=bootstrap_size, replace=True)\n",
    "                boot_df = new_df.iloc[choices]\n",
    "\n",
    "                c_index = cph.score(boot_df, scoring_method=\"concordance_index\")\n",
    "                c_indices.append(c_index)\n",
    "\n",
    "            c_indices.sort()\n",
    "            \n",
    "            hi = c_indices[int(0.975*len(c_indices))]\n",
    "            lo = c_indices[int(0.025*len(c_indices))]\n",
    "\n",
    "            std_err = np.std(c_indices, ddof=1) / np.sqrt(num_bootstraps)\n",
    "            t_value = 1.96\n",
    "            ebm = t_value * std_err\n",
    "\n",
    "            results.append(dict(method=name, n_feats=n_feats, cindex=c_main, cindex_opt=c_corrected, error=error, hi=hi, lo=lo, ebm=ebm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results).to_csv('../results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c:\\Users\\bilel.guetarni\\anaconda3\\envs\\M1_project\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:123: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: There are significant negative eigenvalues (0.0114487 of the maximum positive). Either the matrix is not PSD, or there was an issue while computing the eigendecomposition of the matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c:\\Users\\bilel.guetarni\\anaconda3\\envs\\M1_project\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:589: UserWarning: n_components is too large: it will be set to 170\n",
    "  warnings.warn("
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c:\\Users\\bilel.guetarni\\anaconda3\\envs\\M1_project\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:589: UserWarning: n_components is too large: it will be set to 170\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
